{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMILES_featurization_try_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lBB-oQjDa9xwvLgRahZeBKzyx6AgJzSU",
      "authorship_tag": "ABX9TyOJ8nUCw600o3oOG7yo6yJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veren4/SMILES_featurization/blob/master/SMILES_featurization_try_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtOzr6JeTTZy",
        "colab_type": "text"
      },
      "source": [
        "This notebook is based on an official AllenNLP example: https://allennlp.org/tutorials \\\n",
        "[Second, more detailed tutorial](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktbo1UgggHDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install SmilesPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Cer_5OTfrG",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp76-i0UN_Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Iterator, List, Dict         # type annotations\n",
        "\n",
        "import torch                                    # AllenNLP is built on PyTorch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from allennlp.data import Instance              # training sample = Instance containing fields"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6OGeoVQPx4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.fields import TextField, SequenceLabelField      # possible fields: http://docs.allennlp.org/v0.9.0/api/allennlp.data.fields.html"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H02BqLbCS47v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.dataset_readers import DatasetReader     # reads a file and produces a stream of Instances"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lakQkm8TFNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data as a cached_path??\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1lX3mV3DBYiMxp4xICvUt4fxRyeNSpwN7'\n",
        "downloaded = drive.CreateFile({'id': file_id})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVybX5b4e7L9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "f7268c16-9190-443a-a1c1-a608c1907d23"
      },
      "source": [
        "print(downloaded.GetContentString())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C(C(C(COP(=O)(O)O)O)O)C(=O)C(=O)O\n",
            "CCC(C)C(=O)C(=O)O\n",
            "CC(C)(COP(=O)(O)OP(=O)(O)OCC1C(C(C(O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)C(C(=O)NCCC(=O)NCCSC(=O)CC(CCC(=O)O)O)O\n",
            "C1=CC(=CC(=C1)O)CCC(=O)O\n",
            "COC1=CC2=C(C=CN=C2C=C1)C(C3CC4CCN3CC4(C=C)O)O\n",
            "C(CC(=O)O)C(=O)CC(=O)O\n",
            "CC(C)(COP(=O)(O)OP(=O)(O)OCC1C(C(C(O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)C(C(=O)NCCC(=O)NCCSC(=O)CC(=O)CCC(=O)O)O\n",
            "C(C(C(C(=O)C(C(=O)O)O)O)O)O\n",
            "CC(=O)CC(=O)O\n",
            "C(C1C(C(C(C(O1)O)O)O)OC2C(C(=O)C(C(O2)CO)O)O)O\n",
            "C(C(=O)C(=O)O)S\n",
            "CC(=CC(=O)SCCNC(=O)CCNC(=O)C(C(C)(C)COP(=O)(O)OP(=O)(O)OCC1C(C(C(O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O)C\n",
            "CC(=CCCC(=CCCC(=CCCC(=CCCC(=CCCC(=CCCC(=CCCC(=CCC1=C(C=CC(=C1)C(=O)O)O)C)C)C)C)C)C)C)C\n",
            "C1=CC(=CC(=C1)O)C=O\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeRDje-ATMxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer     # Tokenindexer: rule for how to turn a token into indices\n",
        "from allennlp.data.tokenizers import Token\n",
        "\n",
        "# The Tokenizer I used before:\n",
        "from SmilesPE.pretokenizer import atomwise_tokenizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDc3OIifV3os",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "709c73cf-9433-421a-b4dc-64843f6eb499"
      },
      "source": [
        "from allennlp.data.vocabulary import Vocabulary     # mapping from strings -> integers\n",
        "\n",
        "from allennlp.models import Model                   # a PyTorch Module\n",
        "                                                    # input: tensors\n",
        "                                                    # output: dict of tensor output (including the training loss)\n",
        "\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "\n",
        "from allennlp.training.metrics import CategoricalAccuracy       # for tracking accuracy on the training and validation datasets\n",
        "                                                                # accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
        "\n",
        "from allennlp.data.iterators import BucketIterator      # for creating (mini?)batches\n",
        "\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "from allennlp.predictors import SentenceTaggerPredictor # make predictions on new input"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-66fcb794d1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                                 \u001b[0;31m# accuracy = (TP+TN)/(TP+FP+TN+FN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasicIterator\u001b[0m      \u001b[0;31m# for creating (mini?)batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp.data.iterators'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST8fKTFNg3Ik",
        "colab_type": "text"
      },
      "source": [
        "Iterator: Batches the data \\\n",
        "Trainer: Handles training and metric recording \\\n",
        "(Predictor: Generates predictions from raw strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwEAwihsTlcA",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tatEr9psU5H5",
        "colab_type": "text"
      },
      "source": [
        "Achtung: Ich muss bei dem PubChem sample file die Zeilennummerierung wegschneiden!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGvR0qlOV1Kk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06775da6-7f8d-49d6-da6f-8507061f2fc1"
      },
      "source": [
        "torch.manual_seed(1)   # Set random seed manually to replicate results"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5023d59570>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNZew5hagjD6",
        "colab_type": "text"
      },
      "source": [
        "DatasetReader: Extracts necessary information from data into a list of Instance objects \\\n",
        "1. Reading the data from disk\n",
        "2. Extracting relevant information from the data\n",
        "3. Converting the data into a list of Instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYAx0FD-YjEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SMILES_tokens_DatasetReader(DatasetReader):\n",
        "\n",
        "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None):\n",
        "        super().__init__(lazy=False)\n",
        "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}      # unique ID for each distinct token\n",
        "\n",
        "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
        "        sentence_field = TextField(tokens, self.token_indexers)\n",
        "        fields = {\"sentence\": sentence_field}\n",
        "\n",
        "        if tags:\n",
        "            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n",
        "            fields[\"labels\"] = label_field\n",
        "\n",
        "        return Instance(fields)\n",
        "\n",
        "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
        "        with open(file_path) as f:\n",
        "            for line in f:\n",
        "                pairs = line.strip().split()\n",
        "                sentence, tags = zip(*(pair.split(\"###\") for pair in pairs))\n",
        "                yield self.text_to_instance([Token(word) for word in sentence], tags)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIA2JfVPguVX",
        "colab_type": "text"
      },
      "source": [
        "Model: The model to be trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grn9pfQhYqa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmTagger(Model):                # subclass of the torch.nn.Module\n",
        "\n",
        "    def __init__(self,\n",
        "\n",
        "                 word_embeddings: TextFieldEmbedder,\n",
        "\n",
        "                 encoder: Seq2SeqEncoder,\n",
        "\n",
        "                 vocab: Vocabulary) -> None:\n",
        "        super().__init__(vocab)\n",
        "        self.word_embeddings = word_embeddings\n",
        "        self.encoder = encoder\n",
        "\n",
        "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                          out_features=vocab.get_vocab_size('labels'))\n",
        "\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "    def forward(self,\n",
        "                sentence: Dict[str, torch.Tensor],\n",
        "                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "        mask = get_text_field_mask(sentence)\n",
        "\n",
        "        embeddings = self.word_embeddings(sentence)\n",
        "\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "\n",
        "        tag_logits = self.hidden2tag(encoder_out)\n",
        "        output = {\"tag_logits\": tag_logits}\n",
        "\n",
        "        if labels is not None:\n",
        "            self.accuracy(tag_logits, labels, mask)\n",
        "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBCI9J1kToBt",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSiXQNvxTpmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reader = PosDatasetReader()                     # create an instance of the DatasetReader\n",
        "\n",
        "train_dataset = reader.read(cached_path(\n",
        "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
        "    '/master/tutorials/tagger/training.txt'))\n",
        "validation_dataset = reader.read(cached_path(\n",
        "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
        "    '/master/tutorials/tagger/validation.txt'))\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n",
        "\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "\n",
        "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
        "\n",
        "model = LstmTagger(word_embeddings, lstm, vocab)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = 0\n",
        "\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "\n",
        "    cuda_device = -1\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
        "\n",
        "iterator.index_with(vocab)\n",
        "\n",
        "trainer = Trainer(model=model,              # instantiate the trainer\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  patience=10,\n",
        "                  num_epochs=1000,\n",
        "                  cuda_device=cuda_device)\n",
        "\n",
        "trainer.train()                             # run the trainer\n",
        "\n",
        "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
        "\n",
        "tag_logits = predictor.predict(\"The dog ate the apple\")['tag_logits']\n",
        "\n",
        "tag_ids = np.argmax(tag_logits, axis=-1)\n",
        "\n",
        "print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGF-m8NFYwEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor2 = SentenceTaggerPredictor(model2, dataset_reader=reader)\n",
        "tag_logits2 = predictor2.predict(\"The dog ate the apple\")['tag_logits']\n",
        "np.testing.assert_array_almost_equal(tag_logits2, tag_logits)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}