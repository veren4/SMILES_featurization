{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "#                                                                                             #\n",
    "# This script throws out rows in the dataset where the SMILES contains tokens which aren't in #\n",
    "# the Lenselink alphabet.                                                                     #     \n",
    "#                                                                                             #\n",
    "###############################################################################################\n",
    "\n",
    "dataset_filepath = '../datasets/PubChem/CID-SMILES'   # Attention: Needs adaptation if the file has a filetype extension!\n",
    "\n",
    "alphabet_lense = {'[N-]', '[C@@]', '[C@]', ')', '#', '2', '7', 'F', 'o', 'I', 'O', '[O-]', '/', 'P', 'c', 'C', '(', '[O]', 's', 'Cl', '[N+]', '6', '[n+]', '1', '=', '-', '5', 'N', '\\\\', '[S+]', 'S', '4', '[nH]', '[C@@H]', 'Br', 'n', '3', '[C@H]'}\n",
    "\n",
    "\n",
    "######################################## adapt parameters above ########################################\n",
    "\n",
    "import os\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "import re\n",
    "\n",
    "# generate and open an output file\n",
    "#cleaned_des = open(str(os.path.basename(dataset_filepath) + '_cleaned'), 'a')\n",
    "cleaned_ds = open('../datasets/PubChem/CID-SMILES_cleaned_for_Lense_alphabet', 'a')\n",
    "\n",
    "with open(dataset_filepath) as ds:\n",
    "\n",
    "    for index, line in enumerate(iterable=ds):\n",
    "\n",
    "        current_search = re.search('^[0-9]+\\t(.+)$', line)\n",
    "        if current_search:\n",
    "            current_smiles = current_search.group(1)\n",
    "            current_tokens = atomwise_tokenizer(current_smiles)\n",
    "            \n",
    "            if set(current_tokens).issubset(alphabet_lense):\n",
    "                cleaned_ds.write(line)   \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "cleaned_ds.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4 GB -> cleaning -> 5.6 GB\n",
    "\n",
    "before: 103276515 lines\\\n",
    "after: 92985059 lines       -> I lost 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = 103276515\n",
    "after = 92985059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.03504717408406"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(after/before)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
