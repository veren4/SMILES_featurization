{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_KDNuggets_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lutdoxer6t3c",
        "FO-qNNhI6vNH"
      ],
      "mount_file_id": "1iKw-D2ViV19wm2IEiabl0B24AyRStdOM",
      "authorship_tag": "ABX9TyMZJticktzsRsjrasD7T1AL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veren4/SMILES_featurization/blob/master/LSTM_KDNuggets_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My--lJljomTg"
      },
      "source": [
        "This notebook ist based on [this tutorial](https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html). The code is from their Github repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEadlnoYTiRk"
      },
      "source": [
        "Open problems with this model:\n",
        "* Wenn ich in dem Satz, den ich hinten reinfüttere zum Predicten, ein Zeichen habe, das im Trainings-Datensatz nicht vorkam, kriege ich einen Fehler. => Generell muss ich unknown tokens einführen.\n",
        "* Ich schaue das Vokabular des ganzen Datensatzes an. Wenn ich den aber am Anfang nicht einlese, geht das nicht => Vorher bestimmen und hier nur einlesen!\n",
        "* Datensatz einlesen, ohne komplett in den Cache zu laden ([Massive Dataset class](https://github.com/pytorch/text/issues/130))\n",
        "* Adapt lstm size and embedding dim?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba4fQd45ROHm"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGkOXzWZRPMW"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from torch import nn, optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "!pip install -q SmilesPE\n",
        "from SmilesPE.pretokenizer import atomwise_tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k65BqjmuO3Vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4a42f8-1467-4e16-c8ab-62bdeb872a0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW79DPFWTA_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1a2981-b34d-4d0f-cde5-52a3de101469"
      },
      "source": [
        "import platform\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Python: ', platform.python_version())\n",
        "print('PyTorch: ', torch.__version__)\n",
        "if(device.type == 'cuda'):\n",
        "  print('Using GPU (cuda)')\n",
        "else:\n",
        "  print('Using CPU!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python:  3.6.9\n",
            "PyTorch:  1.7.0+cu101\n",
            "Using GPU (cuda)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foLfnpyy6sKa"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG-t2-hs6xMF"
      },
      "source": [
        "#import torch\n",
        "#import pandas as pd\n",
        "#from collections import Counter\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        args,\n",
        "    ):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = ['UNK', 'SOL', 'EOL', 'PAD', '1', 'N', ')', 'C', 'S', '=', '4', 'O', '(', '2', '3', 'P']\n",
        "\n",
        "        # tokenization dictionaries (numerization)\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        # numericalize all the tokens\n",
        "        #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "        # if the token is in word_to index, then the index, otherwise 'UNK' = 0\n",
        "        # [f(x) if condition else g(x) for x in sequence]\n",
        "        #self.words_indexes = [self.word_to_index[w] if (w in uniq_words) else self.word_to_index['UNK'] for w in self.words]\n",
        "        self.words_indexes = [self.word_to_index[w] if (w in self.uniq_words) else 0 for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "\n",
        "        #train_df = pd.read_csv('data/worded_smiles.csv')\n",
        "        infile = '/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/worded_smiles.csv'\n",
        "        with open(infile, \"r\") as file1:\n",
        "            train_df = pd.read_csv(file1)\n",
        "        file1.close()\n",
        "\n",
        "        # Tokenize\n",
        "        train_df['tokenized_SMILES'] = ''\n",
        "        for row in range(train_df.shape[0]):\n",
        "          train_df.loc[row, 'tokenized_SMILES'] = atomwise_tokenizer(train_df.loc[row, 'SMILES'])\n",
        "        \n",
        "        # Padding + SOL + EOL\n",
        "        for row in range(train_df.shape[0]):              # ATTENTION: Are the column indexes correct?!\n",
        "          actual_length = len(train_df.loc[row, 'tokenized_SMILES'])\n",
        "          length_before_delimiters = self.args.sequence_length - 2\n",
        "\n",
        "          if actual_length > length_before_delimiters:\n",
        "            train_df.loc[row, 'tokenized_SMILES'] = train_df.loc[row, 'tokenized_SMILES'][:length_before_delimiters]\n",
        "            train_df.loc[row, 'tokenized_SMILES'].append('EOL')\n",
        "          elif actual_length < length_before_delimiters:\n",
        "            temp = ['UNK']*length_before_delimiters\n",
        "            shortie = train_df.loc[row, 'tokenized_SMILES']\n",
        "            shortie.append('EOL')\n",
        "            temp[:actual_length] = shortie\n",
        "            train_df.loc[row, 'tokenized_SMILES'] = temp\n",
        "          train_df.loc[row, 'tokenized_SMILES'].insert(0, 'SOL')\n",
        "          \n",
        "\n",
        "        # return the whole dataset as 1 list of tokens\n",
        "        total_token_list = []\n",
        "        for row in range(train_df.shape[0]):\n",
        "          total_token_list.extend(train_df.loc[row, 'tokenized_SMILES'])      # can be combined with above.\n",
        "        return total_token_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),        # turn into cuda tensor?\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
        "        )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I60-yAVuIlza"
      },
      "source": [
        "dataset = Dataset(args)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkEcE7PAgBfF",
        "outputId": "aaca53a9-a0e7-49c1-e74e-3fd0213891bf"
      },
      "source": [
        "dataset.args.sequence_length"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSwPScmRg7nO"
      },
      "source": [
        "args:\\\n",
        "batch_size = 3 (for now; later: try 256)\\\n",
        "max_epochs = 10\\\n",
        "sequence_length = 200\n",
        "\n",
        "I need to pad accordingly! SOL + 200 + EOL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5KHUJiwQwm3"
      },
      "source": [
        "#dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "dataloader = DataLoader(dataset, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoNPUDN8TaXv"
      },
      "source": [
        "type(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnS8RSQhSfe4"
      },
      "source": [
        "for batch, (x, y) in enumerate(dataloader):\n",
        "\n",
        "            #optimizer.zero_grad()\n",
        "\n",
        "            #y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            #loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            #state_h = state_h.detach()\n",
        "            #state_c = state_c.detach()\n",
        "\n",
        "            #loss.backward()\n",
        "            #optimizer.step()\n",
        "\n",
        "            print('batch: ', batch, '(x, y):', (x, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP2DEy_CIoqN"
      },
      "source": [
        "class MassiveDataset(Dataset):\n",
        "    def __init__(self, data_path, line_to_instance, dataset_metadata):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            data_path:          path to file with data.\n",
        "            line_to_instance:   a method converting a line of a file\n",
        "                                to a dataset instance\n",
        "            dataset_metadata:   information required to imitate an in-memory \n",
        "                                dataset: length, offset_dict\n",
        "        \"\"\"\n",
        "        #self.args = args\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "        \n",
        "        \n",
        "        #data_path = '/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/worded_smiles.csv'\n",
        "        self.data_path = data_path\n",
        "        # should be reset in __iter__\n",
        "        self.data_stream = open(data_path, 'r')\n",
        "        self.current_offset = 0\n",
        "\n",
        "        self.meta = dataset_metadata\n",
        "        self.line_to_instance = line_to_instance   # convert the line of the file to a dataset instance\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta['length'])\n",
        "        # original code:\n",
        "        # return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, line):\n",
        "        offset = self.meta['offset_dict'][line]     # absolute position in the file\n",
        "        self.data_stream.seek(offset)       # sets the file's current position at the offset.\n",
        "        line = self.data_stream.readline()\n",
        "        instance = self.line_to_instance(line)\n",
        "        # reset to previous location for iteration\n",
        "        self.data_stream.seek(self.current_offset)\n",
        "        \n",
        "        \n",
        "        \n",
        "        return instance\n",
        "        \n",
        "        # original code:\n",
        "        #        return (\n",
        "        #    torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "        #    torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
        "        #)\n",
        "\n",
        "    def __next__(self):\n",
        "        line = self.data_stream.readline()\n",
        "        self.current_offset = self.data_stream.tell()\n",
        "        return self.line_to_instance(line)\n",
        "        \n",
        "    def load_words(self):\n",
        "        # return all possible tokens -> a predetermined [datastructure]\n",
        "        \n",
        "        \n",
        "        #text = train_df['Joke'].str.cat(sep=' ')\n",
        "        #return text.split(' ')\n",
        "        pass\n",
        "        \n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "        \n",
        "       \n",
        "        \n",
        "    # Sould I close self.data_stream somewhere?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA57QY_QJpKA"
      },
      "source": [
        "massive_dataset = MassiveDataset(\n",
        "    data_path='/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/worded_smiles.csv',\n",
        "    line_to_instance=bla,\n",
        "    dataset_metadata=bla\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoFT2KcZkAzS"
      },
      "source": [
        "    massive_dataloader = DataLoader(\n",
        "        massive_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lutdoxer6t3c"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "82fmlGvU7EZZ",
        "outputId": "044bd179-4ad5-4012-89c1-ae7c1017508d"
      },
      "source": [
        "#import torch\n",
        "#from torch import nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3 #1\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)       # provide predermined number here!\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2#,  #0,\n",
        "            #bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n",
        "                \n",
        "'''\n",
        "    def get_hidden_state(self, x):\n",
        "        out, (h_n, c_n) = self.lstm(embed, prev_state)      # problem: usually, I provide the entire dataset as an input here. That is not possible. => I need to extract this information at the end of the training.\n",
        "        return h_n, c_n\n",
        "        '''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    def get_hidden_state(self, x):\\n        out, (h_n, c_n) = self.lstm(embed, prev_state)      # problem: usually, I provide the entire dataset as an input here. That is not possible. => I need to extract this information at the end of the training.\\n        return h_n, c_n\\n        '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO-qNNhI6vNH"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA38pWgV7IfS"
      },
      "source": [
        "#import argparse\n",
        "#import torch\n",
        "#import numpy as np\n",
        "#from torch import nn, optim\n",
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, model, args):\n",
        "    model.train()       # set the model to training mode\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "        state_h, state_c = model.init_state(args.sequence_length)\n",
        "\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
        "    \n",
        "    #return state_h, state_c\n",
        "\n",
        "def predict(dataset, model, text, next_words=100):   # use this function to output the states at all timesteps!\n",
        "    words = text.split(' ')\n",
        "    model.eval()\n",
        "\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "        \n",
        "    return words"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLUaOcnx7HSa"
      },
      "source": [
        "#Apply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI3Dh4fajbWs"
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--max-epochs', type=int, default=10)\n",
        "parser.add_argument('--batch-size', type=int, default=256)\n",
        "parser.add_argument('--sequence-length', type=int, default=150)\n",
        "args = parser.parse_args(\"\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6YP7w00r8Cq"
      },
      "source": [
        "Change the dataset in dataset.py.\\\n",
        "Set max epochs, batch size, seqence length in train.py."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_g3Ap3WfcX9"
      },
      "source": [
        "dataset = Dataset(args)\n",
        "model = Model(dataset)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vvgyDIWjoQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8deffca-4975-48b9-d5b4-65f00c66b0cf"
      },
      "source": [
        "train(dataset, model, args)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'batch': 0, 'loss': 2.6882803440093994}\n",
            "{'epoch': 0, 'batch': 1, 'loss': 2.647362470626831}\n",
            "{'epoch': 0, 'batch': 2, 'loss': 2.466413736343384}\n",
            "{'epoch': 0, 'batch': 3, 'loss': 2.492624044418335}\n",
            "{'epoch': 0, 'batch': 4, 'loss': 1.986050009727478}\n",
            "{'epoch': 0, 'batch': 5, 'loss': 1.7159440517425537}\n",
            "{'epoch': 0, 'batch': 6, 'loss': 2.1603097915649414}\n",
            "{'epoch': 0, 'batch': 7, 'loss': 1.3696824312210083}\n",
            "{'epoch': 1, 'batch': 0, 'loss': 1.027949571609497}\n",
            "{'epoch': 1, 'batch': 1, 'loss': 1.5320022106170654}\n",
            "{'epoch': 1, 'batch': 2, 'loss': 0.9575214385986328}\n",
            "{'epoch': 1, 'batch': 3, 'loss': 1.754170298576355}\n",
            "{'epoch': 1, 'batch': 4, 'loss': 0.6899461150169373}\n",
            "{'epoch': 1, 'batch': 5, 'loss': 0.7947619557380676}\n",
            "{'epoch': 1, 'batch': 6, 'loss': 1.9124035835266113}\n",
            "{'epoch': 1, 'batch': 7, 'loss': 1.09309983253479}\n",
            "{'epoch': 2, 'batch': 0, 'loss': 0.8300540447235107}\n",
            "{'epoch': 2, 'batch': 1, 'loss': 1.3994698524475098}\n",
            "{'epoch': 2, 'batch': 2, 'loss': 0.8111574053764343}\n",
            "{'epoch': 2, 'batch': 3, 'loss': 1.5620462894439697}\n",
            "{'epoch': 2, 'batch': 4, 'loss': 0.5637475252151489}\n",
            "{'epoch': 2, 'batch': 5, 'loss': 0.6702683568000793}\n",
            "{'epoch': 2, 'batch': 6, 'loss': 1.7906712293624878}\n",
            "{'epoch': 2, 'batch': 7, 'loss': 0.9854486584663391}\n",
            "{'epoch': 3, 'batch': 0, 'loss': 0.7317373752593994}\n",
            "{'epoch': 3, 'batch': 1, 'loss': 1.2769399881362915}\n",
            "{'epoch': 3, 'batch': 2, 'loss': 0.7149659991264343}\n",
            "{'epoch': 3, 'batch': 3, 'loss': 1.4269007444381714}\n",
            "{'epoch': 3, 'batch': 4, 'loss': 0.4926164150238037}\n",
            "{'epoch': 3, 'batch': 5, 'loss': 0.5905193090438843}\n",
            "{'epoch': 3, 'batch': 6, 'loss': 1.5903202295303345}\n",
            "{'epoch': 3, 'batch': 7, 'loss': 0.8258569240570068}\n",
            "{'epoch': 4, 'batch': 0, 'loss': 0.6500150561332703}\n",
            "{'epoch': 4, 'batch': 1, 'loss': 1.1351141929626465}\n",
            "{'epoch': 4, 'batch': 2, 'loss': 0.6190196871757507}\n",
            "{'epoch': 4, 'batch': 3, 'loss': 1.3051525354385376}\n",
            "{'epoch': 4, 'batch': 4, 'loss': 0.43187499046325684}\n",
            "{'epoch': 4, 'batch': 5, 'loss': 0.5276737213134766}\n",
            "{'epoch': 4, 'batch': 6, 'loss': 1.40899658203125}\n",
            "{'epoch': 4, 'batch': 7, 'loss': 0.7155924439430237}\n",
            "{'epoch': 5, 'batch': 0, 'loss': 0.5910263657569885}\n",
            "{'epoch': 5, 'batch': 1, 'loss': 1.0442817211151123}\n",
            "{'epoch': 5, 'batch': 2, 'loss': 0.5759258270263672}\n",
            "{'epoch': 5, 'batch': 3, 'loss': 1.2005711793899536}\n",
            "{'epoch': 5, 'batch': 4, 'loss': 0.39066082239151}\n",
            "{'epoch': 5, 'batch': 5, 'loss': 0.48829174041748047}\n",
            "{'epoch': 5, 'batch': 6, 'loss': 1.326811671257019}\n",
            "{'epoch': 5, 'batch': 7, 'loss': 0.6808327436447144}\n",
            "{'epoch': 6, 'batch': 0, 'loss': 0.557365357875824}\n",
            "{'epoch': 6, 'batch': 1, 'loss': 1.0040662288665771}\n",
            "{'epoch': 6, 'batch': 2, 'loss': 0.5557183027267456}\n",
            "{'epoch': 6, 'batch': 3, 'loss': 1.151971697807312}\n",
            "{'epoch': 6, 'batch': 4, 'loss': 0.3645267188549042}\n",
            "{'epoch': 6, 'batch': 5, 'loss': 0.4622024595737457}\n",
            "{'epoch': 6, 'batch': 6, 'loss': 1.2859636545181274}\n",
            "{'epoch': 6, 'batch': 7, 'loss': 0.6528939008712769}\n",
            "{'epoch': 7, 'batch': 0, 'loss': 0.5294129252433777}\n",
            "{'epoch': 7, 'batch': 1, 'loss': 0.9636246562004089}\n",
            "{'epoch': 7, 'batch': 2, 'loss': 0.5288534760475159}\n",
            "{'epoch': 7, 'batch': 3, 'loss': 1.1130316257476807}\n",
            "{'epoch': 7, 'batch': 4, 'loss': 0.3410486876964569}\n",
            "{'epoch': 7, 'batch': 5, 'loss': 0.4384456276893616}\n",
            "{'epoch': 7, 'batch': 6, 'loss': 1.255318284034729}\n",
            "{'epoch': 7, 'batch': 7, 'loss': 0.6242175698280334}\n",
            "{'epoch': 8, 'batch': 0, 'loss': 0.5102342963218689}\n",
            "{'epoch': 8, 'batch': 1, 'loss': 0.9380092620849609}\n",
            "{'epoch': 8, 'batch': 2, 'loss': 0.5107446312904358}\n",
            "{'epoch': 8, 'batch': 3, 'loss': 1.0976359844207764}\n",
            "{'epoch': 8, 'batch': 4, 'loss': 0.3317207992076874}\n",
            "{'epoch': 8, 'batch': 5, 'loss': 0.4269312620162964}\n",
            "{'epoch': 8, 'batch': 6, 'loss': 1.2220577001571655}\n",
            "{'epoch': 8, 'batch': 7, 'loss': 0.6006230115890503}\n",
            "{'epoch': 9, 'batch': 0, 'loss': 0.5010988116264343}\n",
            "{'epoch': 9, 'batch': 1, 'loss': 0.9181829690933228}\n",
            "{'epoch': 9, 'batch': 2, 'loss': 0.5060014724731445}\n",
            "{'epoch': 9, 'batch': 3, 'loss': 1.0721068382263184}\n",
            "{'epoch': 9, 'batch': 4, 'loss': 0.320030152797699}\n",
            "{'epoch': 9, 'batch': 5, 'loss': 0.4146498739719391}\n",
            "{'epoch': 9, 'batch': 6, 'loss': 1.1979771852493286}\n",
            "{'epoch': 9, 'batch': 7, 'loss': 0.5821459889411926}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_J6Ro_VdEPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5038ed98-c3ad-4422-a0e2-db599e578a42"
      },
      "source": [
        "print(predict(dataset, model, text='C C ( O 2 ) C'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['C', 'C', '(', 'O', '2', ')', 'C', 'P', '4', '2', '(', 'SOL', '3', '=', ')', ')', '(', '=', 'O', 'N', '(', 'O', 'C', ')', '(', '=', 'SOL', 'N', 'O', 'O', 'C', '=', 'O', 'C', '=', 'O', 'PAD', 'S', '=', ')', ')', 'C', '(', 'O', ')', '2', 'C', '(', 'O', 'O', ')', 'O', '(', '=', '(', 'C', 'C', 'EOL', 'N', ')', 'N', 'O', 'C', '(', 'P', 'EOL', 'O', 'C', '1', 'O', 'O', ')', ')', '3', '1', 'C', 'O', 'O', 'O', ')', ')', 'C', ')', 'O', '=', '(', ')', 'O', ')', '(', ')', 'C', 'C', 'O', 'C', '(', 'C', 'C', 'C', '=', ')', ')', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz8lHz3bKuIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "cc8c358f-42d5-48de-ba58-8ed7ed7f0438"
      },
      "source": [
        "state_h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f8dcaf89e15b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'state_h' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UnIhq1DkcQi"
      },
      "source": [
        "state_h.d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09aPRHo8Kwz1"
      },
      "source": [
        "state_c"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}