{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_KDNuggets_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ba4fQd45ROHm",
        "vjS2BqJGL4ra",
        "PYzR3R-bXPz9",
        "zRlODAX7FHvO",
        "-HW0WBdpaQOl",
        "lutdoxer6t3c"
      ],
      "mount_file_id": "1iKw-D2ViV19wm2IEiabl0B24AyRStdOM",
      "authorship_tag": "ABX9TyPvOoqOIHVXf4OU71WSlS7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veren4/SMILES_featurization/blob/master/LSTM_KDNuggets_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My--lJljomTg"
      },
      "source": [
        "This notebook ist based on [this tutorial](https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html). The code is from their Github repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEadlnoYTiRk"
      },
      "source": [
        "Open problems with this model:\n",
        "* Wenn ich in dem Satz, den ich hinten reinfüttere zum Predicten, ein Zeichen habe, das im Trainings-Datensatz nicht vorkam, kriege ich einen Fehler. => Generell muss ich unknown tokens einführen.\n",
        "* Ich schaue das Vokabular des ganzen Datensatzes an. Wenn ich den aber am Anfang nicht einlese, geht das nicht => Vorher bestimmen und hier nur einlesen!\n",
        "* Datensatz einlesen, ohne komplett in den Cache zu laden ([Massive Dataset class](https://github.com/pytorch/text/issues/130))\n",
        "* Adapt lstm size and embedding dim?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba4fQd45ROHm"
      },
      "source": [
        "#Setup RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGkOXzWZRPMW"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from torch import nn, optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "!pip install -q SmilesPE\n",
        "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
        "import pickle\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k65BqjmuO3Vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2c2baa-de26-4c01-f377-1e2e931ac832"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW79DPFWTA_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7d839c-0bd6-4472-980e-63a20d956556"
      },
      "source": [
        "import platform\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Python: ', platform.python_version())\n",
        "print('PyTorch: ', torch.__version__)\n",
        "if(device.type == 'cuda'):\n",
        "  print('Using GPU (cuda)')\n",
        "else:\n",
        "  print('Using CPU!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python:  3.6.9\n",
            "PyTorch:  1.7.0+cu101\n",
            "Using GPU (cuda)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjS2BqJGL4ra"
      },
      "source": [
        "#Metdata RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYPqmgK0L9Rs"
      },
      "source": [
        "* Number of lines\n",
        "* regex: cut away line number + tabs/spaces\n",
        "* byte offset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXSZkNcSyAFd"
      },
      "source": [
        "wc -l CID-SMILES\\\n",
        "108826964 CID-SMILES\n",
        "\n",
        "108 826 964   lines are in that file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTAl_w-AZudM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "3211c79a-0e1d-4d0b-9797-fda85a33115a"
      },
      "source": [
        "'''\n",
        "data_stream = open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/CID-SMILES-first100', 'r')\n",
        "#data_stream = open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/CID-SMILES', 'r')\n",
        "#data_stream = open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/worded_smiles_no_header.csv', 'r')\n",
        "\n",
        "offset_list = list()\n",
        "\n",
        "number_of_lines = 100\n",
        "#number_of_lines = 108826964\n",
        "#number_of_lines = 14\n",
        "\n",
        "for i in range(number_of_lines):\n",
        "  data_stream.readline()\n",
        "  offset_list.append(data_stream.tell())\n",
        "\n",
        "data_stream.close()\n",
        "\n",
        "with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list_CID-first100.pkl', 'wb') as fid:\n",
        "#with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list.pkl', 'wb') as fid:\n",
        "#with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list_worded_smiles_no_header.pkl', 'wb') as fid:\n",
        "\n",
        "  pickle.dump(offset_list, fid)\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndata_stream = open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/CID-SMILES-first100', 'r')\\n#data_stream = open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/CID-SMILES', 'r')\\n#data_stream = open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/worded_smiles_no_header.csv', 'r')\\n\\noffset_list = list()\\n\\nnumber_of_lines = 100\\n#number_of_lines = 108826964\\n#number_of_lines = 14\\n\\nfor i in range(number_of_lines):\\n  data_stream.readline()\\n  offset_list.append(data_stream.tell())\\n\\ndata_stream.close()\\n\\nwith open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list_CID-first100.pkl', 'wb') as fid:\\n#with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list.pkl', 'wb') as fid:\\n#with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list_worded_smiles_no_header.pkl', 'wb') as fid:\\n\\n  pickle.dump(offset_list, fid)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHcfV-T51zrO"
      },
      "source": [
        "with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list_CID-first100.pkl', 'rb') as fid:\n",
        "     offset_list = pickle.load(fid)\n",
        "\n",
        "#with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list.pkl', 'rb') as fid:\n",
        "#     offset_list = pickle.load(fid)\n",
        "\n",
        "#with open('/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/offset_list_worded_smiles_no_header.pkl', 'rb') as fid:\n",
        "#     offset_list = pickle.load(fid)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjhbrHK4XLNf"
      },
      "source": [
        "dict_offset = { i : offset_list[i] for i in range(0, len(offset_list) ) }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foLfnpyy6sKa"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYzR3R-bXPz9"
      },
      "source": [
        "##Standard Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG-t2-hs6xMF"
      },
      "source": [
        "#import torch\n",
        "#import pandas as pd\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        args,\n",
        "    ):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = ['UNK', 'SOL', 'EOL', 'PAD', '1', 'N', ')', 'C', 'S', '=', '4', 'O', '(', '2', '3', 'P']\n",
        "\n",
        "        # tokenization dictionaries (numerization)\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        # numericalize all the tokens\n",
        "        #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "        # if the token is in word_to index, then the index, otherwise 'UNK' = 0\n",
        "        # [f(x) if condition else g(x) for x in sequence]\n",
        "        #self.words_indexes = [self.word_to_index[w] if (w in uniq_words) else self.word_to_index['UNK'] for w in self.words]\n",
        "        self.words_indexes = [self.word_to_index[w] if (w in self.uniq_words) else 0 for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "\n",
        "        #train_df = pd.read_csv('data/worded_smiles.csv')\n",
        "        infile = '/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/worded_smiles.csv'\n",
        "        with open(infile, \"r\") as file1:\n",
        "            train_df = pd.read_csv(file1)\n",
        "        file1.close()\n",
        "\n",
        "        # Tokenize\n",
        "        train_df['tokenized_SMILES'] = ''\n",
        "        for row in range(train_df.shape[0]):\n",
        "          train_df.loc[row, 'tokenized_SMILES'] = atomwise_tokenizer(train_df.loc[row, 'SMILES'])\n",
        "        \n",
        "        # Padding + SOL + EOL\n",
        "        for row in range(train_df.shape[0]):              # ATTENTION: Are the column indexes correct?!\n",
        "          actual_length = len(train_df.loc[row, 'tokenized_SMILES'])\n",
        "          length_before_delimiters = self.args.sequence_length - 2\n",
        "\n",
        "          if actual_length > length_before_delimiters:\n",
        "            train_df.loc[row, 'tokenized_SMILES'] = train_df.loc[row, 'tokenized_SMILES'][:length_before_delimiters]\n",
        "            train_df.loc[row, 'tokenized_SMILES'].append('EOL')\n",
        "          elif actual_length < length_before_delimiters:\n",
        "            temp = ['UNK']*length_before_delimiters\n",
        "            shortie = train_df.loc[row, 'tokenized_SMILES']\n",
        "            shortie.append('EOL')\n",
        "            temp[:actual_length] = shortie\n",
        "            train_df.loc[row, 'tokenized_SMILES'] = temp\n",
        "          train_df.loc[row, 'tokenized_SMILES'].insert(0, 'SOL')\n",
        "          \n",
        "        # return the whole dataset as 1 list of tokens\n",
        "        total_token_list = []\n",
        "        for row in range(train_df.shape[0]):\n",
        "          total_token_list.extend(train_df.loc[row, 'tokenized_SMILES'])      # can be combined with above.\n",
        "        return total_token_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args.sequence_length   # !!!!\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),  # input      # turn into cuda tensor?\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),    # label\n",
        "        )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRlODAX7FHvO"
      },
      "source": [
        "##Standard DS Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWb7z_4c_TY-"
      },
      "source": [
        "0 = 'UNK'\\\n",
        "1 = 'SOL'\\\n",
        "2 = 'EOL'\\\n",
        "\n",
        "Es sollte als jeder Batch mit 1 beginnen, und mit 2 enden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlPbFRlxA46g"
      },
      "source": [
        "dataset = Dataset(args)\n",
        "subset_indices = [0]\n",
        "\n",
        "subset = torch.utils.data.Subset(dataset, subset_indices)\n",
        "\n",
        "#testloader_subset = torch.utils.data.DataLoader(subset, batch_size=args.batch_size, num_workers=0, shuffle=False)\n",
        "testloader_subset = DataLoader(subset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "for batch, (x, y) in enumerate(testloader_subset):\n",
        "            print('\\nbatch: ', batch, '\\n(x, y):', (x, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mxVQsFyAfBR"
      },
      "source": [
        "#dataset.args.batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnS8RSQhSfe4"
      },
      "source": [
        "#for batch, (x, y) in enumerate(dataloader):\n",
        "#            print('\\nbatch: ', batch, '\\n(x, y):', (x, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vfwSV1yFJ8S"
      },
      "source": [
        "##Massive Dataset class RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP2DEy_CIoqN"
      },
      "source": [
        "class Massive_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        args,\n",
        "    ):\n",
        "        #self.args = args\n",
        "        self.offsets = dict_offset  #offset_meta\n",
        "        self.length_of_dataset = 14 #dataset_length\n",
        "        self.data_path = '/content/drive/My Drive/Rostlab internship/8_KDNuggets_LSTM_Approach/data/CID-SMILES-first100'\n",
        "        # should be reset in __iter__\n",
        "        self.data_stream = open(self.data_path, 'r')\n",
        "        self.current_offset = 0\n",
        "\n",
        "        self.uniq_words = ['UNK', 'SOL', 'EOL', '(', ')', 'O', 'S', '1', '#', '6', '=', 'C', 'N', '7', '[N+]', '2', '4', 'P', '[O-]', '3', '5', 'Cl']  # vocab for the first 100 of CID_SMILES!\n",
        "        # tokenization dictionaries (numerization)\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length_of_dataset\n",
        "\n",
        "    def line_to_instance(self, line):\n",
        "        tokenized_SMILES = atomwise_tokenizer(line)\n",
        "        tokenized_SMILES.insert(0, 'SOL')\n",
        "        tokenized_SMILES.append('EOL')\n",
        "        tokenized_SMILES = [self.word_to_index[w] if (w in self.uniq_words) else self.word_to_index['UNK'] for w in tokenized_SMILES]\n",
        "        return tokenized_SMILES\n",
        "\n",
        "    def __getitem__(self, line):\n",
        "        offset = self.offsets[line]        \n",
        "        self.data_stream.seek(offset)\n",
        "        line = self.data_stream.readline()\n",
        "        line = line.rstrip(\"\\n\")\n",
        "        match = re.match(pattern = \"^\\d+\\\\t(.+)$\", string = line)\n",
        "        if match:\n",
        "          line = match.group(1)\n",
        "        else:\n",
        "          raise ValueError('I couldn\\'t find a SMILES in a line of the dataset. :(')\n",
        "\n",
        "        instance = self.line_to_instance(line)\n",
        "        label_instance = instance[1:]\n",
        "        label_instance.append(0)    # Currently, the last token is a 0. It is\n",
        "                                    # debatable whether that is suitable, but\n",
        "                                    # I slice it out later aynway, so I just\n",
        "                                    # let that 0 be for now.\n",
        "        \n",
        "        '''\n",
        "        # Padding with -1        \n",
        "        instance += [-1] * (args.sequence_length - len(instance))\n",
        "        label_instance += [-1] * (args.sequence_length - len(label_instance))\n",
        "        '''\n",
        "        \n",
        "        return (                                                                                   # turn into cuda tensor?\n",
        "            torch.tensor(instance),\n",
        "            torch.tensor(label_instance),  # is the second tensor really used as a label tensor, or is it the other way around?\n",
        "        )\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HW0WBdpaQOl"
      },
      "source": [
        "##Massive DS testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPVBzuqKaShq",
        "outputId": "bf17ad60-608e-4d16-f21d-403a7782dd35"
      },
      "source": [
        "dataset = Massive_Dataset(args)\n",
        "              # offset_length = 100\n",
        "              # offset_meta = dict_offset\n",
        "\n",
        "subset_indices = [0]  # [0,1,2,3,4]\n",
        "\n",
        "subset = torch.utils.data.Subset(dataset, subset_indices)\n",
        "\n",
        "#testloader_subset = torch.utils.data.DataLoader(subset, batch_size=args.batch_size, num_workers=0, shuffle=False)\n",
        "testloader_subset = DataLoader(subset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "for batch, (x, y) in enumerate(testloader_subset):\n",
        "            print('\\nbatch: ', batch,\n",
        "                  '\\n(x, y):', (x, y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "batch:  0 \n",
            "(x, y): (tensor([[ 1, 11, 11,  3, 10,  5,  4,  5, 11,  3, 11, 11,  3, 10,  5,  4,  5,  4,\n",
            "         11, 14,  3, 11,  4,  3, 11,  4, 11,  2]]), tensor([[11, 11,  3, 10,  5,  4,  5, 11,  3, 11, 11,  3, 10,  5,  4,  5,  4, 11,\n",
            "         14,  3, 11,  4,  3, 11,  4, 11,  2,  0]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lutdoxer6t3c"
      },
      "source": [
        "#Model RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82fmlGvU7EZZ"
      },
      "source": [
        "#import torch\n",
        "#from torch import nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3 #1\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)       # provide predermined number here!\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2#,  #0,\n",
        "            #bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)  # input_size, hidden_size, num_layers\n",
        "        logits = self.fc(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),    # initializatio with zeros?\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n",
        "                \n",
        "\n",
        "    #def get_hidden_state(self, x):\n",
        "    #    out, (h_n, c_n) = self.lstm(embed, prev_state)      # problem: usually, I provide the entire dataset as an input here. That is not possible. => I need to extract this information at the end of the training.\n",
        "    #    return h_n, c_n\n",
        "        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO-qNNhI6vNH"
      },
      "source": [
        "#Train RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA38pWgV7IfS"
      },
      "source": [
        "#import argparse\n",
        "#import torch\n",
        "#import numpy as np\n",
        "#from torch import nn, optim\n",
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, model, args):\n",
        "    model.train()       # set the model to training mode\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=args.batch_size,\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "        state_h, state_c = model.init_state(args.sequence_length)   # Here, I need the sequence length for initializing the LSTM anew for this epoch?\n",
        "                                                                    # Why is it always initialized with zeros? \n",
        "\n",
        "                                                                    # The argument here must not be args.sequence_length, but the length of the current sequence!\n",
        "                                                                    # Or something else in a batch, so the length of the padded batch, or the length of the\n",
        "                                                                    # concatenated sequences or so?\n",
        "\n",
        "        #state_h, state_c = model.init_state(x.size()[1])\n",
        "\n",
        "\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "\n",
        "            print(\"Type of x: \", type(x), \"\\n\")\n",
        "            print(\"x: \", x, \"\\n\")\n",
        "            print(\"dims of x: \", x.size(), \"\\n\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            '''\n",
        "            S1/ this is a sequence\n",
        "            S2/ a sequence\n",
        "\n",
        "            [\n",
        "            [-1, 0, 1, 2, 3, -2], [0, 1, 2, 3, -2, -1][/2]\n",
        "            [-1, 2, 3, -2, None, None], [2, 3, -2, -1, None, None][/2]\n",
        "            ]\n",
        "\n",
        "            -- in training this is\n",
        "            [-1, 0, 1, 2, 3, -2, -1, 2, 3, -2,] [0, 1, 2, 3, -2, -1, 2, 3, -2, -1,]\n",
        "\n",
        "            [-1, 0, 1, 2, 3, -2, -1, -1, 2, 3, -2,] [0, 1, 2, 3, -2, -1, -1, 2, 3, -2, -1,]\n",
        "            '''\n",
        "            # here: slice x and y, leave out the last token (don't predict stuff after)\n",
        "\n",
        "            # I padded with -1\n",
        "            # Attention: This slicing needs to be adapted for batch size >1!\n",
        "            '''\n",
        "            numpy_version = x.numpy()\n",
        "            padding_indices = np.where(numpy_version == -1)\n",
        "            while -1 in numpy_version:\n",
        "              numpy_version = numpy_version[:(padding_indices[0][0]-1)]\n",
        "            x = torch.tensor(numpy_version)\n",
        "\n",
        "            numpy_version = y.numpy()\n",
        "            padding_indices = np.where(numpy_version == -1)\n",
        "            while -1 in numpy_version:\n",
        "              numpy_version = numpy_version[:(padding_indices[0][0]-1)]\n",
        "            y = torch.tensor(numpy_version)\n",
        "            '''\n",
        "\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
        "    \n",
        "\n",
        "def predict(dataset, model, text, next_words=100):   # use this function to output the states at all timesteps!\n",
        "    words = text.split(' ')\n",
        "    model.eval()\n",
        "\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "        \n",
        "    return words"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLUaOcnx7HSa"
      },
      "source": [
        "#Apply RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI3Dh4fajbWs"
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--max-epochs', type=int, default=10)\n",
        "parser.add_argument('--batch-size', type=int, default=1)\n",
        "parser.add_argument('--sequence-length', type=int, default=300) # pad with Nones, which i slice out later.\n",
        "args = parser.parse_args(\"\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6YP7w00r8Cq"
      },
      "source": [
        "Change the dataset in dataset.py.\\\n",
        "Set max epochs, batch size, seqence length in train.py."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_g3Ap3WfcX9"
      },
      "source": [
        "#dataset = Dataset(args)\n",
        "dataset = Massive_Dataset(args)\n",
        "\n",
        "model = Model(dataset)   # to cuda"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rsx2f0GmaLD7",
        "outputId": "8aac3ed4-403f-4e5f-f345-0904ae2d0ef8"
      },
      "source": [
        "train(dataset, model, args)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of x:  <class 'torch.Tensor'> \n",
            "\n",
            "x:  tensor([[ 1, 11, 11,  3, 10,  5,  4,  5, 11,  3, 11, 11,  3, 10,  5,  4,  5,  4,\n",
            "         11, 14,  3, 11,  4,  3, 11,  4, 11,  2]]) \n",
            "\n",
            "dims of x:  torch.Size([1, 28]) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f1bb79de07bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-bfeb89962757>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5bdb67e7ac7e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# input_size, hidden_size, num_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 534\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    535\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    536\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (3, 28, 128), got [3, 300, 128]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMqZW8ok3bxn"
      },
      "source": [
        "https://stackoverflow.com/questions/57148617/runtimeerror-expected-hidden-size-2-24-50-got-2-30-50\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_J6Ro_VdEPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4b3e54-281d-47dd-e4d7-a731b813ab8e"
      },
      "source": [
        "print(predict(dataset, model, text='C C ( O 2 ) C'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['C', 'C', '(', 'O', '2', ')', 'C', '(', '=', 'C', '1', ')', 'O', ')', 'C', '=', 'O', 'EOL', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'SOL', 'C', 'C', '(', '=', 'C', 'C', '(', '=', 'C', '1', ')', 'O', ')', 'C', '=', 'O', 'EOL', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'SOL', 'C', 'C', 'C', '(', '=', 'C', 'C', '(', '=', 'C', '1', ')', 'O', ')', 'C', '=', 'O', 'EOL', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id7H9cuPaf2-"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rqr6_PgmZ0E"
      },
      "source": [
        "!nvidia-smi "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrzBCq20maJY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}