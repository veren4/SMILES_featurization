{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "F5qMQSA1V2Ce",
        "4W5gBF6w7wFI"
      ],
      "authorship_tag": "ABX9TyMFQMZVu0YaMDk5vGhPMWkY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veren4/SMILES_featurization/blob/master/My_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKVTR7ZOtoG6"
      },
      "source": [
        "[Tutorial](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj7Ej_GNW5Oh",
        "outputId": "f6608b65-66aa-4576-df24-cda4a5bfa71c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pickle\n",
        "\n",
        "infile1 = open('/content/drive/My Drive/Rostlab internship/7_PyTorch/Tokenized_Dataset', 'rb')\n",
        "tokenized_dataset = pickle.load(infile1)\n",
        "infile1.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5qMQSA1V2Ce"
      },
      "source": [
        "###First Trial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awRlmIc6sxEe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTjVPArAs2Kl"
      },
      "source": [
        "input_dim = 5 #3         # size of the input at each step\n",
        "hidden_dim = 10 #3       # size of the hidden state and cell state at each step\n",
        "n_layers = 1          # number of LSTM layers stacked on top of each other\n",
        "\n",
        "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8JpQRFtYRY"
      },
      "source": [
        "batch_size = 1\n",
        "seq_len = 1\n",
        "\n",
        "#inp = torch.randn(batch_size, seq_len, input_dim)\n",
        "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "hidden = (hidden_state, cell_state)    # tuple"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLHU3aLRXHCB",
        "outputId": "7d54b03a-3b2f-49be-bb50-d89f13b51c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(inp)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiE6H7JTMTp6",
        "outputId": "1fe767c3-29dd-4567-c948-230d7fdc0d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(\"Input:\", inp.size())\n",
        "print(\"Hidden state:\", hidden_state.size())\n",
        "print(\"Cell state:\", cell_state.size())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: torch.Size([1, 1, 5])\n",
            "Hidden state: torch.Size([1, 1, 10])\n",
            "Cell state: torch.Size([1, 1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SKam5glOBh6"
      },
      "source": [
        "Now let's see what comes out of the LSTM layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFspZ3iNxKUA",
        "outputId": "bcbd44cd-bbd9-4ef2-9d40-2e1f14639a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "out, hidden = lstm_layer(inp, hidden)\n",
        "print(\"Output shape: \", out.shape)\n",
        "print(\"Hidden: \", hidden)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output shape:  torch.Size([1, 1, 10])\n",
            "Hidden:  (tensor([[[-0.4495,  0.1912,  0.1779,  0.4032, -0.1397,  0.3387, -0.1074,\n",
            "          -0.4542, -0.2520,  0.3526]]], grad_fn=<StackBackward>), tensor([[[-0.6424,  0.3893,  0.2377,  1.1927, -0.2825,  0.8477, -0.4810,\n",
            "          -0.7275, -0.5518,  0.7644]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie-QsAMTDkvB"
      },
      "source": [
        "##Data preparation (int_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWmCtv3dDux6"
      },
      "source": [
        "####Create vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxA8dwQpxQPN",
        "outputId": "e6175f06-7f4f-46ea-accd-b3cba3f12d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#len(tokenized_dataset)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y66-J_eDsNe"
      },
      "source": [
        "alphabet = set()\n",
        "\n",
        "for i in tokenized_dataset:\n",
        "  alphabet.update(i)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FklgBhnhEQKV",
        "outputId": "d7d0bc21-a3d0-4c4e-9496-657f592053ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "alphabet    # length: 12"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(', ')', '1', '2', '3', '4', '=', 'C', 'N', 'O', 'P', 'S'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnZRAbk7sQNW"
      },
      "source": [
        "####Create indices to vectorize words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V7mqxxOz_fO",
        "outputId": "7d571662-dace-476e-855f-9cfc4e7c617e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "dict_token_alphabet = {}\n",
        "dict_token_alphabet.update({'Padding': 0, 'Unknown': 1})\n",
        "\n",
        "index = 2\n",
        "for i in alphabet:\n",
        "  dict_token_alphabet.update({i: index})\n",
        "  index = index+1\n",
        "\n",
        "dict_token_alphabet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(': 9,\n",
              " ')': 13,\n",
              " '1': 5,\n",
              " '2': 10,\n",
              " '3': 4,\n",
              " '4': 7,\n",
              " '=': 6,\n",
              " 'C': 8,\n",
              " 'N': 11,\n",
              " 'O': 12,\n",
              " 'P': 3,\n",
              " 'Padding': 0,\n",
              " 'S': 2,\n",
              " 'Unknown': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjroB_Y70xrj"
      },
      "source": [
        "####Vectorize SMILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvZgE_fa04pA"
      },
      "source": [
        "#tokenized_dataset[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlIdYoQL1AAR"
      },
      "source": [
        "int_tokens = [None]*len(tokenized_dataset)    # empty list of length 14\n",
        "\n",
        "for i in range(len(tokenized_dataset)):\n",
        "  int_tokens[i] = [None]*len(tokenized_dataset[i])\n",
        "\n",
        "  for j in range(len(tokenized_dataset[i])):\n",
        "    int_tokens[i][j] = dict_token_alphabet.get(tokenized_dataset[i][j])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kmDkHHC1VO1",
        "outputId": "5611c7ee-929d-4a2a-99fa-7ef327a9043a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#int_tokens[1]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 8, 8, 9, 8, 13, 8, 9, 6, 12, 13, 8, 9, 6, 12, 13, 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP9pAqqOeDXU"
      },
      "source": [
        "####Batching + Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puGrOKuEaxDx"
      },
      "source": [
        "[Tutorial](https://github.com/chrisvdweth/ml-toolkit/blob/master/pytorch/notebooks/minimal-example-lstm-input.ipynb)\\\n",
        "Still open: What is a batch for me? For now, I will go with 3 SMILES.\\\n",
        "The sequence length has to be constant within 1 batch. -> Pad accordingly\\\n",
        "Choose Batches with equal length so that I have to do minimal Padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oe-UCQEa3Ka"
      },
      "source": [
        "batch = [int_tokens[0], int_tokens[1], int_tokens[3]] \n",
        "\n",
        "pad_to_this = len(max(batch, key=len))    #  length of the longest list in the batch -> All other elements need to be padded to this length"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toWZVL72iY_5"
      },
      "source": [
        "pad_beginning = True\n",
        "\n",
        "for SMILES in batch:\n",
        "  while(len(SMILES) < pad_to_this):\n",
        "    if(pad_beginning == True):\n",
        "      SMILES.insert(0, 0)\n",
        "      pad_beginning = !pad_beginning\n",
        "    else:\n",
        "      SMILES.append(0)\n",
        "      pad_beginning = !pad_beginning"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-wHOTGdlC1x"
      },
      "source": [
        "#for SMILES in batch:\n",
        "#  print(len(SMILES))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BezuSOixlVTJ"
      },
      "source": [
        "float_batch = [[float(token) for token in SMILES] for SMILES in batch]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWptnErF3eoO"
      },
      "source": [
        "##Putting my data into a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W5gBF6w7wFI"
      },
      "source": [
        "####Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X-xTo8Q7vPG"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1KW_D0z7z64",
        "outputId": "b7dfd0b2-bb60-43db-fe3f-4681ee904e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import platform\n",
        "print('Using python: ', platform.python_version())\n",
        "print('Using torch version: ', torch.__version__)\n",
        "print('Using device: ', device)\n",
        "# Machine: 2015 13\" Macbook Pro, i5 dual core\n",
        "\n",
        "import torch.nn as nn\n",
        "import timeit"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using python:  3.6.9\n",
            "Using torch version:  1.6.0+cu101\n",
            "Using device:  cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9e0I0dpVsit"
      },
      "source": [
        "####Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-sb8M0X73Vv"
      },
      "source": [
        "# choose the input parameters\n",
        "input_size = 33         # max length of SMILES (token sequence)\n",
        "hidden_dim = 1   # What does this parameter do?\n",
        "seq_len= 2  # What does this parameter do?\n",
        "\n",
        "# define the model\n",
        "pytorch_lstm=nn.LSTM(input_size, hidden_dim)\n",
        "\n",
        "# initialise the lstm\n",
        "for p in pytorch_lstm.parameters():\n",
        "    nn.init.constant_(p, val=0.3)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW7ILgdG3cwu",
        "outputId": "d8cdcf49-deac-4137-ed4d-1357633749ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#X = torch.randn(seq_len, 1, input_size)\n",
        "X = torch.tensor([float_batch])\n",
        "X"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 8.,  9.,  8.,  9.,  8.,  9.,  8., 12.,  3.,  9.,  6., 12., 13.,  9.,\n",
              "          12., 13., 12., 13., 12., 13., 12., 13.,  8.,  9.,  6., 12., 13.,  8.,\n",
              "           9.,  6., 12., 13., 12.],\n",
              "         [ 0.,  0.,  8.,  8.,  8.,  9.,  8., 13.,  8.,  9.,  6., 12., 13.,  8.,\n",
              "           9.,  6., 12., 13., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "           0.,  0.,  0.,  0.,  0.],\n",
              "         [ 8.,  5.,  6.,  8.,  8.,  9.,  6.,  8.,  8.,  9.,  6.,  8.,  5., 13.,\n",
              "          12., 13.,  8.,  8.,  8.,  9.,  6., 12., 13., 12.,  0.,  0.,  0.,  0.,\n",
              "           0.,  0.,  0.,  0.,  0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ITxePTIQrv"
      },
      "source": [
        "output_pytorch, (h_pytorch, _) = pytorch_lstm(X)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnWGxp28cWa",
        "outputId": "4f188884-df0b-4ac5-e874-c529e9201815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "runs=10**4\n",
        "\n",
        "print(\"Time Pytorch LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_lstm(X)\", \n",
        "                                       setup=\"from __main__ import pytorch_lstm, X\", \n",
        "                                       number=runs))\n",
        "     )"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Pytorch LSTM 10000 runs: 1.476s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI9M5yyA818q",
        "outputId": "06bcb548-2df1-4703-9125-b522947d3452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#X.size()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}