{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "A2CuADTRvLg5",
        "MWmCtv3dDux6",
        "FnZRAbk7sQNW"
      ],
      "authorship_tag": "ABX9TyNS9FWr0GERsDnlwwF16qFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veren4/SMILES_featurization/blob/master/My_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlF6WsexaFaF"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVNv91KcaKQb",
        "outputId": "f142c5ba-97a3-4e5a-f977-84621110253b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "#import torchvision    # data loaders for common datasets such as Imagenet, FashionMNIST, MNIST, etc. and data transformers for images\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnJ6u21AaSiO",
        "outputId": "c812a5a4-bc29-4540-8623-d5214d96ef86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import platform\n",
        "print('Using python: ', platform.python_version())\n",
        "print('Using torch version: ', torch.__version__)\n",
        "print('Using device: ', device)\n",
        "# Machine: 2015 13\" Macbook Pro, i5 dual core\n",
        "\n",
        "#import torch.nn as nn\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timeit"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using python:  3.6.9\n",
            "Using torch version:  1.6.0+cu101\n",
            "Using device:  cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPq8JIb_aiGk",
        "outputId": "dcd6e064-9097-40c8-a4f0-9f671c771bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pickle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V_EVk_OBrXc"
      },
      "source": [
        "import tensorboard\n",
        "#print(f\"Tensorboard version: {tensorboard.__version__}\")\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard.notebook\n",
        "\n",
        "# imports\n",
        "%autoreload 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsFLyDDJBsxi"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY7Qm_oN8Jk6"
      },
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ_ludYuuK6P"
      },
      "source": [
        "###Set up TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7-rj7zZscKD"
      },
      "source": [
        "[How to TensorBoard](https://pytorch.org/docs/stable/tensorboard.html)\\\n",
        "[PyTorch Tensorboard tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\\\n",
        "([Medium article](https://medium.com/@iamsdt/using-tensorboard-in-google-colab-with-pytorch-458f9bb95212) on TensorBoard in Colab with PyTorch) <- no suitable version available\\\n",
        "[The best tutorial](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb#scrollTo=IRSe6eHcFPyT): in a colab notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2CuADTRvLg5"
      },
      "source": [
        "###Seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxCxMTWYGHoB",
        "outputId": "131fc71d-a2fc-4f47-dbb5-cc021da2d146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6b70949570>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHgd-XA5vPP8"
      },
      "source": [
        "https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ien_2TZxaK5h"
      },
      "source": [
        "###Description & documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_WSaelG_wjP"
      },
      "source": [
        "Later: automated batching\\\n",
        "Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKVTR7ZOtoG6"
      },
      "source": [
        "[FloydHub](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)\\\n",
        "[GitHub Minimal example](https://github.com/chrisvdweth/ml-toolkit/blob/master/pytorch/notebooks/minimal-example-lstm-input.ipynb)\\\n",
        "[PyTorch LSTM Beginner guide](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\\\n",
        "[PyTorch LSTM Guide](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\\\n",
        "[Medium article](https://medium.com/@sunitachoudhary103/generating-molecules-using-a-char-rnn-in-pytorch-16885fd9394b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxGjp7MC4Di6"
      },
      "source": [
        "[deeplearningwizard](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/): Steps\n",
        "\n",
        "Step 1: Load Dataset\\\n",
        "Step 2: Make Dataset Iterable\\\n",
        "Step 3: Create Model Class\\\n",
        "Step 4: Instantiate Model Class\\\n",
        "Step 5: Instantiate Loss Class\\\n",
        "Step 6: Instantiate Optimizer Class\\\n",
        "Step 7: Train Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie-QsAMTDkvB"
      },
      "source": [
        "##Data preparation (int_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj7Ej_GNW5Oh"
      },
      "source": [
        "infile1 = open('/content/drive/My Drive/Rostlab internship/7_PyTorch/Tokenized_Dataset', 'rb')\n",
        "tokenized_dataset = pickle.load(infile1)\n",
        "infile1.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWmCtv3dDux6"
      },
      "source": [
        "####Create vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y66-J_eDsNe"
      },
      "source": [
        "alphabet = set()\n",
        "\n",
        "for i in tokenized_dataset:\n",
        "  alphabet.update(i)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FklgBhnhEQKV"
      },
      "source": [
        "#alphabet    # length: 12"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnZRAbk7sQNW"
      },
      "source": [
        "####Token alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V7mqxxOz_fO",
        "outputId": "364b3004-71b3-46ed-a95a-e120f6281aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "dict_token_alphabet = {}\n",
        "dict_token_alphabet.update({'PAD': 0, 'UNK': 1, 'SOS': 2, 'EOS':3})\n",
        "\n",
        "index = 4\n",
        "for i in alphabet:\n",
        "  dict_token_alphabet.update({i: index})\n",
        "  index = index+1\n",
        "\n",
        "dict_token_alphabet"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(': 6,\n",
              " ')': 14,\n",
              " '1': 10,\n",
              " '2': 11,\n",
              " '3': 4,\n",
              " '4': 9,\n",
              " '=': 12,\n",
              " 'C': 15,\n",
              " 'EOS': 3,\n",
              " 'N': 7,\n",
              " 'O': 5,\n",
              " 'P': 13,\n",
              " 'PAD': 0,\n",
              " 'S': 8,\n",
              " 'SOS': 2,\n",
              " 'UNK': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjroB_Y70xrj"
      },
      "source": [
        "####Vectorize SMILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlIdYoQL1AAR"
      },
      "source": [
        "int_tokens = [None]*len(tokenized_dataset)    # empty list of length 14\n",
        "\n",
        "for i in range(len(tokenized_dataset)):\n",
        "  int_tokens[i] = [None]*len(tokenized_dataset[i])\n",
        "\n",
        "  for j in range(len(tokenized_dataset[i])):\n",
        "    int_tokens[i][j] = dict_token_alphabet.get(tokenized_dataset[i][j])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puGrOKuEaxDx"
      },
      "source": [
        "Still open: What is a batch for me? For now, I will go with 3 SMILES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kmDkHHC1VO1"
      },
      "source": [
        "batch = [int_tokens[0], int_tokens[1], int_tokens[3]]\n",
        "\n",
        "pad_to_this = len(max(batch, key=len))    #  length of the longest list in the batch -> All other elements need to be padded to this length\n",
        "\n",
        "pad_beginning = True\n",
        "\n",
        "for SMILES in batch:\n",
        "  while(len(SMILES) < pad_to_this):\n",
        "    if(pad_beginning == True):\n",
        "      SMILES.insert(0, 0)\n",
        "      pad_beginning = !pad_beginning\n",
        "    else:\n",
        "      SMILES.append(0)\n",
        "      pad_beginning = !pad_beginning"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYpWd5WCvWIS",
        "outputId": "94ae460b-52e3-4ab2-a3f1-64e9fc502e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# make a numpy array out of it\n",
        "batch = np.array(batch)\n",
        "\n",
        "# make a PyTorch tensor\n",
        "batch = torch.tensor(batch, dtype=torch.long)\n",
        "\n",
        "print(batch)\n",
        "print('The shape of batch is:', batch.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[15,  6, 15,  6, 15,  6, 15,  5, 13,  6, 12,  5, 14,  6,  5, 14,  5, 14,\n",
            "          5, 14,  5, 14, 15,  6, 12,  5, 14, 15,  6, 12,  5, 14,  5],\n",
            "        [ 0, 15, 15, 15,  6, 15, 14, 15,  6, 12,  5, 14, 15,  6, 12,  5, 14,  5,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [15, 10, 12, 15, 15,  6, 12, 15, 15,  6, 12, 15, 10, 14,  5, 14, 15, 15,\n",
            "         15,  6, 12,  5, 14,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "The shape of batch is: torch.Size([3, 33])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-pIKaFEwQVh"
      },
      "source": [
        "Now we have the first step of having our data in the shape (batch_size, seq_len). to feed it into an LSTM, we still need input_size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OVvhX7WwBIE"
      },
      "source": [
        "####Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keULFvHEw4RM"
      },
      "source": [
        "Define layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NenQIp9dwFUL"
      },
      "source": [
        "vocab_size = len(dict_token_alphabet)  # 14\n",
        "embed_dim = 10 #<- What does the size of the embeddings mean in my case?\n",
        "\n",
        "word_embedding_layer = nn.Embedding(vocab_size, embed_dim)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIV42R9qw57B"
      },
      "source": [
        "Push batch through layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG5Pd99qwie8",
        "outputId": "de78326f-e1ce-476c-c343-45f7007ae5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch = word_embedding_layer(batch)\n",
        "\n",
        "print('The shape of batch is:', batch.shape)   # 3, 33, 10\n",
        "#print()\n",
        "#print(batch)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of batch is: torch.Size([3, 33, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFSlKKrsxJz8"
      },
      "source": [
        "Now we have want we want: (batch_size, seq_len, input_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dpS7bI6G9Nh"
      },
      "source": [
        "####Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAbKAXUaHYn1"
      },
      "source": [
        "PyTorch Dataloader:\\\n",
        "[PyTorch guide to its Dataloader class](https://pytorch.org/docs/stable/data.html)\\\n",
        "[Parameters](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
        "\n",
        "\\\n",
        "(Alternative to the PyTorch function:  Function from the [Medium article](https://medium.com/@sunitachoudhary103/generating-molecules-using-a-char-rnn-in-pytorch-16885fd9394b))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz0uD-LQ8m0H",
        "outputId": "90f8d08c-fea1-4688-a034-2b483d9321e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 1\n",
        "n_iters = 20\n",
        "num_epochs = n_iters / (len(tokenized_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "print(num_epochs, 'epochs')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGKhsIruHXhP"
      },
      "source": [
        "my_loader = torch.utils.data.DataLoader(dataset=tokenized_dataset,\n",
        "                                        batch_size=3,\n",
        "                                        shuffle=True,\n",
        "                                        sampler=None,       # optional: custom Sampler object\n",
        "                                        batch_sampler=None, # optional: provide a custom sampler\n",
        "                                        num_workers=0,      # if positive int => multi-process data loading\n",
        "                                        collate_fn=None,    # optional: custom collate function\n",
        "                                        pin_memory=False,   # to speed it up when working on a GPU\n",
        "                                        drop_last=False,\n",
        "                                        timeout=0,\n",
        "                                        worker_init_fn=None)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psWMRbCkug_P"
      },
      "source": [
        "I have a map-style dataset, at least for now with this testing dataset. However, when I take the huge PubChem dataset, this might change to iterable-style.\\\n",
        "When automatic batching is enabled, collate_fn is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWptnErF3eoO"
      },
      "source": [
        "##Putting my data into a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI9M5yyA818q",
        "outputId": "4ca61bbc-7346-446c-db88-8200076e19ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch.size()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 33, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-sb8M0X73Vv"
      },
      "source": [
        "# choose the input parameters\n",
        "input_size = batch.shape[2]\n",
        "hidden_dim = 32\n",
        "\n",
        "# define the model\n",
        "my_LSTM=nn.LSTM(input_size, hidden_dim)\n",
        "\n",
        "# initialise the lstm\n",
        "for parameter in my_LSTM.parameters():\n",
        "    nn.init.normal_(parameter)\n",
        "    #print(parameter)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Q_9l4mLh5_",
        "outputId": "86c51252-576a-423d-f77a-2e77025a3cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# Model architecture visualization\n",
        "writer = SummaryWriter()    # default `log_dir` is \"runs\"\n",
        "\n",
        "writer.add_graph(model=my_LSTM.cpu(),\n",
        "                 input_to_model=batch,\n",
        "                 verbose=True)\n",
        "writer.close()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-60fc95c90503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model architecture visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# default `log_dir` is \"runs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m writer.add_graph(model=my_LSTM.cpu(),\n\u001b[1;32m      5\u001b[0m                  \u001b[0minput_to_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SummaryWriter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2D0RePMqWow"
      },
      "source": [
        "#%tensorboard — writer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrLQwsvCme_q"
      },
      "source": [
        "####Instantiate Loss Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yt9PTizBQXm"
      },
      "source": [
        "#import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv3dM2HM4icS"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK2x4gyg4m40"
      },
      "source": [
        "####Instantiate Optimizer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erTaZkNZ4wix"
      },
      "source": [
        "# minibatch SGD\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(my_LSTM.parameters(), lr=learning_rate)  "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI0KzlHp5Vs9",
        "outputId": "6b933b66-e68c-45d7-c850-0435297883d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#for i in range(len(list(my_LSTM.parameters()))):\n",
        "#    print(list(my_LSTM.parameters())[i].size())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 10])\n",
            "torch.Size([128, 32])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1FQydbIBBZQ"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ITxePTIQrv"
      },
      "source": [
        "lstm_out, hidden = my_LSTM(batch)\n",
        "\n",
        "print('The shape of lstm_out is:', lstm_out.shape) # (seq_len, batch_size, hidden_dim)\n",
        "print('The shape of hidden is:', hidden.shape) # (num_layers*num_directions, batch_size, hidden_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXEhfDtv0cwm"
      },
      "source": [
        "import timeit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnWGxp28cWa",
        "outputId": "d363db1b-9e17-4f85-8746-bd303861f0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "runs=10**4\n",
        "\n",
        "print(\"Time Pytorch LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"my_LSTM(batch)\", \n",
        "                                       setup=\"from __main__ import my_LSTM, batch\", \n",
        "                                       number=runs))\n",
        "     )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Pytorch LSTM 10000 runs: 18.690s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_EsPhh87Hx"
      },
      "source": [
        "[Optuna](https://github.com/optuna/optuna): automated Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9yRS4Vx9A2U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc8H338EBqZY"
      },
      "source": [
        "##Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb7DvNoKFrsJ"
      },
      "source": [
        "Let's write some stuff to TensorBoard, and log into it to see how things go :)\n",
        "You can log into TensorBoard by running the command from this exercise folder in Terminal:\n",
        "\n",
        "```tensorboard --logdir=runs```\n",
        "\n",
        "For Linux user, you can use open a Terminal and simply run it\n",
        "\n",
        "For Windows user with Anaconda, you may open an Anaconda Prompt and then run the command. Otherwise use your default setup of running python code in cmd.\n",
        "\n",
        "Note that before running the command **you may get into root folder of this Notebook first**. Finally from the command line and then navigating to http://localhost:6006. If everything went well you will be presented with the tensorboard stup and after executing the next cell you should see the following images in TensorBoard.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnX5-BG4FX8z",
        "outputId": "223a072c-bf0f-4918-ed0f-2edfef6cbc49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# write to tensorboard\n",
        "#writer.add_image('four_mnist_images', img_grid)     # I need to write my data here!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-215867d9be08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# write to tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'four_mnist_images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_grid\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# I need to write my data here!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'img_grid' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFiGnqQWF3XJ",
        "outputId": "2e5a1fe2-70df-4224-b739-a47a9e0b188b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "# Model architecture visualization\n",
        "\n",
        "writer.add_graph(model=word_embedding_layer.cpu(),\n",
        "                 input_to_model=batch,\n",
        "                 verbose=True)\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
            "Error occurs, No graph saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-029555a0e577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m writer.add_graph(model=word_embedding_layer.cpu(),\n\u001b[1;32m      2\u001b[0m                  \u001b[0minput_to_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                  verbose=True)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, model, input_to_model, verbose)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# A valid PyTorch model should have a 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;31m# Caffe2 models do not have the 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error occurs, No graph saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_model_mode_for_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: move outside of torch.onnx?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_inline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    953\u001b[0m         return trace_module(func, {'forward': example_inputs}, None,\n\u001b[1;32m    954\u001b[0m                             \u001b[0mcheck_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                             check_tolerance, strict, _force_outplace, _module_class)\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     if (hasattr(func, '__self__') and isinstance(func.__self__, torch.nn.Module) and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"forward\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_method_from_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_lookup_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdvkUaxIGWul"
      },
      "source": [
        "# Start TensorBoard within the notebook using magics function\n",
        "%tensorboard — logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}