{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "F5qMQSA1V2Ce",
        "4W5gBF6w7wFI"
      ],
      "authorship_tag": "ABX9TyMur1RLA56OmZDs4GtEJ71B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veren4/SMILES_featurization/blob/master/My_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKVTR7ZOtoG6"
      },
      "source": [
        "[Tutorial](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj7Ej_GNW5Oh",
        "outputId": "3c1dbee8-cb49-4b9c-e245-70d12fae5b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pickle\n",
        "\n",
        "infile1 = open('/content/drive/My Drive/Rostlab internship/7_PyTorch/Tokenized_Dataset', 'rb')\n",
        "tokenized_dataset = pickle.load(infile1)\n",
        "infile1.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5qMQSA1V2Ce"
      },
      "source": [
        "###First Trial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awRlmIc6sxEe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTjVPArAs2Kl"
      },
      "source": [
        "input_dim = 5 #3         # size of the input at each step\n",
        "hidden_dim = 10 #3       # size of the hidden state and cell state at each step\n",
        "n_layers = 1          # number of LSTM layers stacked on top of each other\n",
        "\n",
        "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8JpQRFtYRY"
      },
      "source": [
        "batch_size = 1\n",
        "seq_len = 1\n",
        "\n",
        "#inp = torch.randn(batch_size, seq_len, input_dim)\n",
        "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "hidden = (hidden_state, cell_state)    # tuple"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLHU3aLRXHCB",
        "outputId": "c839f759-d43e-4a38-a0c5-7cf773b05895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "type(inp)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ae02192eb55b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'inp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiE6H7JTMTp6"
      },
      "source": [
        "print(\"Input:\", inp.size())\n",
        "print(\"Hidden state:\", hidden_state.size())\n",
        "print(\"Cell state:\", cell_state.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SKam5glOBh6"
      },
      "source": [
        "Now let's see what comes out of the LSTM layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFspZ3iNxKUA"
      },
      "source": [
        "out, hidden = lstm_layer(inp, hidden)\n",
        "print(\"Output shape: \", out.shape)\n",
        "print(\"Hidden: \", hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie-QsAMTDkvB"
      },
      "source": [
        "##Data preparation (int_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWmCtv3dDux6"
      },
      "source": [
        "####Create vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxA8dwQpxQPN"
      },
      "source": [
        "#len(tokenized_dataset)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y66-J_eDsNe"
      },
      "source": [
        "alphabet = set()\n",
        "\n",
        "for i in tokenized_dataset:\n",
        "  alphabet.update(i)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FklgBhnhEQKV",
        "outputId": "5a8d4efe-f01f-4d5d-8e85-c91467158db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "alphabet    # length: 12"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(', ')', '1', '2', '3', '4', '=', 'C', 'N', 'O', 'P', 'S'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnZRAbk7sQNW"
      },
      "source": [
        "####Create indices to vectorize words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V7mqxxOz_fO",
        "outputId": "0be84c5d-6f15-49a0-998d-6a6b680aae9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "dict_token_alphabet = {}\n",
        "dict_token_alphabet.update({'Padding': 0, 'Unknown': 1})\n",
        "\n",
        "index = 2\n",
        "for i in alphabet:\n",
        "  dict_token_alphabet.update({i: index})\n",
        "  index = index+1\n",
        "\n",
        "dict_token_alphabet"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(': 8,\n",
              " ')': 11,\n",
              " '1': 5,\n",
              " '2': 4,\n",
              " '3': 9,\n",
              " '4': 2,\n",
              " '=': 12,\n",
              " 'C': 13,\n",
              " 'N': 7,\n",
              " 'O': 3,\n",
              " 'P': 6,\n",
              " 'Padding': 0,\n",
              " 'S': 10,\n",
              " 'Unknown': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjroB_Y70xrj"
      },
      "source": [
        "####Vectorize SMILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlIdYoQL1AAR"
      },
      "source": [
        "int_tokens = [None]*len(tokenized_dataset)    # empty list of length 14\n",
        "\n",
        "for i in range(len(tokenized_dataset)):\n",
        "  int_tokens[i] = [None]*len(tokenized_dataset[i])\n",
        "\n",
        "  for j in range(len(tokenized_dataset[i])):\n",
        "    int_tokens[i][j] = dict_token_alphabet.get(tokenized_dataset[i][j])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puGrOKuEaxDx"
      },
      "source": [
        "[Tutorial](https://github.com/chrisvdweth/ml-toolkit/blob/master/pytorch/notebooks/minimal-example-lstm-input.ipynb)\\\n",
        "Still open: What is a batch for me? For now, I will go with 3 SMILES.\\\n",
        "The sequence length has to be constant within 1 batch. -> Pad accordingly\\\n",
        "Choose Batches with equal length so that I have to do minimal Padding. -> Is there a function for this like in AllenNLP?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kmDkHHC1VO1"
      },
      "source": [
        "batch = [int_tokens[0], int_tokens[1], int_tokens[3]]\n",
        "\n",
        "pad_to_this = len(max(batch, key=len))    #  length of the longest list in the batch -> All other elements need to be padded to this length\n",
        "\n",
        "pad_beginning = True\n",
        "\n",
        "for SMILES in batch:\n",
        "  while(len(SMILES) < pad_to_this):\n",
        "    if(pad_beginning == True):\n",
        "      SMILES.insert(0, 0)\n",
        "      pad_beginning = !pad_beginning\n",
        "    else:\n",
        "      SMILES.append(0)\n",
        "      pad_beginning = !pad_beginning"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m6gfEMQuet3"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYpWd5WCvWIS",
        "outputId": "62f12e5f-2ab3-4a65-bc56-75e3b9f48a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# make a numpy array out of it\n",
        "batch = np.array(batch)\n",
        "\n",
        "# make a PyTorch tensor\n",
        "batch = torch.tensor(batch, dtype=torch.long)\n",
        "\n",
        "print(batch)\n",
        "print('The shape of batch is:', batch.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[13,  8, 13,  8, 13,  8, 13,  3,  6,  8, 12,  3, 11,  8,  3, 11,  3, 11,\n",
            "          3, 11,  3, 11, 13,  8, 12,  3, 11, 13,  8, 12,  3, 11,  3],\n",
            "        [ 0, 13, 13, 13,  8, 13, 11, 13,  8, 12,  3, 11, 13,  8, 12,  3, 11,  3,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13,  5, 12, 13, 13,  8, 12, 13, 13,  8, 12, 13,  5, 11,  3, 11, 13, 13,\n",
            "         13,  8, 12,  3, 11,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "The shape of batch is: torch.Size([3, 33])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-pIKaFEwQVh"
      },
      "source": [
        "Now we have the first step of having our data in the shape (batch_size, seq_len). to feed it into an LSTM, we still need input_size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OVvhX7WwBIE"
      },
      "source": [
        "####Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_BBJzDxklW",
        "outputId": "11e347f2-12a5-47f2-d26b-ab6914a05e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f15ea712600>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keULFvHEw4RM"
      },
      "source": [
        "Define layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NenQIp9dwFUL"
      },
      "source": [
        "vocab_size = len(dict_token_alphabet)  # 14\n",
        "embed_dim = 10 #<- What does the size of the embeddings mean in my case?\n",
        "\n",
        "word_embedding_layer = nn.Embedding(vocab_size, embed_dim)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIV42R9qw57B"
      },
      "source": [
        "Push batch through layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG5Pd99qwie8"
      },
      "source": [
        "batch = word_embedding_layer(batch)\n",
        "\n",
        "print('The shape of batch is:', batch.shape)   # 3, 33, 10\n",
        "print()\n",
        "print(batch)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFSlKKrsxJz8"
      },
      "source": [
        "Now we have want we want: (batch_size, seq_len, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BezuSOixlVTJ"
      },
      "source": [
        "# brauche ich wahrscheinlich nicht:\n",
        "#float_batch = [[float(token) for token in SMILES] for SMILES in batch]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWptnErF3eoO"
      },
      "source": [
        "##Putting my data into a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W5gBF6w7wFI"
      },
      "source": [
        "####Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X-xTo8Q7vPG"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1KW_D0z7z64",
        "outputId": "b7dfd0b2-bb60-43db-fe3f-4681ee904e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import platform\n",
        "print('Using python: ', platform.python_version())\n",
        "print('Using torch version: ', torch.__version__)\n",
        "print('Using device: ', device)\n",
        "# Machine: 2015 13\" Macbook Pro, i5 dual core\n",
        "\n",
        "import torch.nn as nn\n",
        "import timeit"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using python:  3.6.9\n",
            "Using torch version:  1.6.0+cu101\n",
            "Using device:  cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9e0I0dpVsit"
      },
      "source": [
        "####Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI9M5yyA818q",
        "outputId": "f69af2d1-2774-465b-8789-1c8aeff0f971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#batch.size()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 33, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-sb8M0X73Vv"
      },
      "source": [
        "# choose the input parameters\n",
        "input_size = batch.shape[2] #33         # max length of SMILES (token sequence)\n",
        "hidden_dim = 32   # What does this parameter do?\n",
        "seq_len= batch.shape[0] #2  # What does this parameter do?\n",
        "\n",
        "# define the model\n",
        "my_LSTM=nn.LSTM(input_size, hidden_dim)\n",
        "\n",
        "# initialise the lstm\n",
        "for p in my_LSTM.parameters():\n",
        "    nn.init.constant_(p, val=0.3)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhgtQ57X0Jhq"
      },
      "source": [
        "# We need to reshape the batch from (batch_size, seq_len, input_size) to (seq_len, batch_size, input_size)\n",
        "batch = batch.transpose(0,1)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ITxePTIQrv",
        "outputId": "123861ef-45c9-4d55-8eb4-405a2173ef3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "lstm_out, (h_pytorch, _) = my_LSTM(batch)\n",
        "\n",
        "print('The shape of lstm_out is:', lstm_out.shape) # (seq_len, batch_size, hidden_dim)\n",
        "print('The shape of h is:', h_pytorch.shape) # (num_layers*num_directions, batch_size, hidden_dim)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of lstm_out is: torch.Size([33, 3, 32])\n",
            "The shape of h is: torch.Size([1, 3, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXEhfDtv0cwm"
      },
      "source": [
        "import timeit"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnWGxp28cWa",
        "outputId": "d363db1b-9e17-4f85-8746-bd303861f0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "runs=10**4\n",
        "\n",
        "print(\"Time Pytorch LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"my_LSTM(batch)\", \n",
        "                                       setup=\"from __main__ import my_LSTM, batch\", \n",
        "                                       number=runs))\n",
        "     )"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Pytorch LSTM 10000 runs: 18.690s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPJ2CWs20eWA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}